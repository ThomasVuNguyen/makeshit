# BeeWalker LSTM Training â€” Walking Style Evolution

**Run:** `lstm_20260207_013141` | **Steps:** 24.9M | **Best Reward:** 669.8 | **Duration:** ~38 hours

This document traces the robot's walking strategy evolution across 2,489 recorded videos. Frames were sampled at 25 key milestones from 10K to 24.9M steps.

## Training Progress

![Training reward curve and episode length over 24.9M steps](images/progress.png)

- **Reward:** 70 â†’ 669.8 (best), stabilizing around 350â€“450
- **Episode length:** 108 â†’ 255 steps (out of 1000 max)
- **Training speed:** ~269â€“2,500 sps (varies with video recording)

---

## Phase 1: "The Statue" â€” Standing Still
**Steps:** 0â€“100K | **Reward:** 70â€“100 | **Strategy:** Don't fall

The model's first discovery: staying upright earns the height and upright bonuses. The robot stands rigidly, locked legs, zero forward movement. It's playing it safeâ€”collecting free reward just by existing.

|  Start of episode | Mid-episode â€” slight wobble |
|:---:|:---:|
| ![](images/10000_01.png) | ![](images/10000_03.png) |

At 100K, it's gained more confidence in standing but still hasn't moved:

| 100K start | 100K mid â€” subtle lean |
|:---:|:---:|
| ![](images/100000_01.png) | ![](images/100000_03.png) |

ğŸ“¹ **Video:** [step_000010000.mp4](videos/step_000010000.mp4)

**What the LSTM is learning:** Balance equilibrium. The hidden state encodes "I am upright" and outputs near-zero actions to maintain it.

---

## Phase 2: "Weight Shifter" â€” Leaning & Wobbling
**Steps:** 100Kâ€“500K | **Reward:** 100â€“150 | **Strategy:** Any forward velocity = bonus reward

The model discovers the velocity reward component. It starts leaning forward and shifting weight, producing small amounts of forward velocity without actually lifting feet. Like someone standing on a bus, swaying slightly.

| 500K start â€” slight forward lean | 500K mid â€” weight shifting, body tilting |
|:---:|:---:|
| ![](images/500000_01.png) | ![](images/500000_03.png) |

ğŸ“¹ **Video:** [step_000500000.mp4](videos/step_000500000.mp4)

**What the LSTM is learning:** The temporal memory starts tracking lean angle over time â€” it learns that sustained lean produces sustained velocity reward.

---

## Phase 3: "Controlled Fall" â€” Falling Forward as locomotion
**Steps:** 500Kâ€“2M | **Reward:** 120â€“200 | **Strategy:** Maximize velocity by tumbling

The classic RL exploit. The model realizes that **falling forward generates massive velocity**. It deliberately tips forward, legs splaying out, collecting huge instantaneous velocity rewards before the height penalty kicks in. The episodes are short but high-scoring.

| 1M start â€” still upright at first | 1M mid â€” collapsed forward, splatted |
|:---:|:---:|
| ![](images/1000000_01.png) | ![](images/1000000_03.png) |

| 2M start â€” learns to reset upright | 2M mid â€” controlled forward tumble |
|:---:|:---:|
| ![](images/2000000_01.png) | ![](images/2000000_03.png) |

ğŸ“¹ **Video:** [step_001000000.mp4](videos/step_001000000.mp4)

**What the LSTM is learning:** Sequencing â€” the hidden state learns "lean â†’ fall â†’ get velocity." But crucially, it also starts encoding the negative signal: falling ends episodes early, cutting off future reward.

---

## Phase 4: "Crouching Shuffle" â€” First Real Leg Movement
**Steps:** 2Mâ€“7M | **Reward:** 200â€“300 | **Strategy:** Stay low, move legs

The breakthrough transition. The model figures out that a **low center of gravity** prevents falling while still allowing forward motion. It adopts a crouched, sumo-like stance and begins alternating leg movements â€” the first signs of actual stepping.

| 3M â€” crouched forward, legs wide apart | 5M start â€” attempts upright start |
|:---:|:---:|
| ![](images/3000000_03.png) | ![](images/5000000_01.png) |

| 5M mid â€” drops into sliding crouch | 7M â€” deeper crouch, active leg cycling |
|:---:|:---:|
| ![](images/5000000_03.png) | ![](images/7000000_03.png) |

ğŸ“¹ **Video:** [step_005000000.mp4](videos/step_005000000.mp4)

**What the LSTM is learning:** Periodic patterns. The hidden state begins encoding a gait cycle â€” alternating left/right leg activation. This is the first time the LSTM's memory offers a real advantage over a stateless MLP.

---

## Phase 5: "The Strider" â€” Genuine Walking
**Steps:** 7Mâ€“16M | **Reward:** 300â€“500 | **Strategy:** Coordinated stepping gait

The model develops a **real walking motion**. Legs show coordinated hip-knee flexion/extension, the torso leans forward at a walking angle, and the robot takes deliberate steps. The episode lengths increase significantly (to 250+ steps), meaning it can sustain movement much longer.

| 10M start â€” confident upright start | 10M mid â€” active stepping, bent knees |
|:---:|:---:|
| ![](images/10000000_01.png) | ![](images/10000000_03.png) |

| 14M â€” fluid forward stride | 16M start â€” stable upright stance |
|:---:|:---:|
| ![](images/14000000_03.png) | ![](images/16000000_01.png) |

| 16M mid â€” walking with upper body coordination |
|:---:|
| ![](images/16000000_03.png) |

ğŸ“¹ **Video:** [step_010000000.mp4](videos/step_010000000.mp4) | [step_016000000.mp4](videos/step_016000000.mp4)

**What the LSTM is learning:** Multi-step planning. The hidden state now encodes a full gait cycle (which leg is next, current phase in the cycle, anticipated balance shifts). It can "look ahead" 2-3 steps in its internal planning.

---

## Phase 6: "Aggressive Walker" â€” Speed Over Stability
**Steps:** 16Mâ€“24.9M | **Reward:** 350â€“670 | **Strategy:** Push velocity, accept falls

The model has learned to walk and now optimizes for **maximum speed**. It develops an aggressive, forward-tilted gait that generates high velocities. The trade-off: it's less stable, frequently losing balance after fast runs. The reward variance is high (350â€“670) because some episodes achieve great runs while others end in early collapse.

| 20M start â€” stable upright | 20M mid â€” aggressive forward lean, fast stepping |
|:---:|:---:|
| ![](images/20000000_01.png) | ![](images/20000000_03.png) |

| 24M start â€” beginning stride | 24M mid â€” falling after speed push |
|:---:|:---:|
| ![](images/24000000_01.png) | ![](images/24000000_03.png) |

| 24.9M start â€” final recorded pose | 24.9M mid â€” wide-stance aggressive gait |
|:---:|:---:|
| ![](images/24890000_01.png) | ![](images/24890000_03.png) |

ğŸ“¹ **Video:** [step_020000000.mp4](videos/step_020000000.mp4) | [step_024890000.mp4](videos/step_024890000.mp4)

**What the LSTM is learning:** Speed-stability tradeoff. The hidden state encodes "how fast am I going" and "am I about to fall" â€” but hasn't yet found the optimal balance point.

---

## Summary

| # | Phase | Steps | Reward | Style | Key Behavior |
|---|-------|-------|--------|-------|-------------|
| 1 | Statue | 0â€“100K | 70â€“100 | ğŸ§ | Rigid standing, no movement |
| 2 | Weight Shifter | 100Kâ€“500K | 100â€“150 | ğŸ”€ | Leaning & wobbling, no foot lift |
| 3 | Controlled Fall | 500Kâ€“2M | 120â€“200 | ğŸ«ƒ | Forward tumble for velocity |
| 4 | Crouching Shuffle | 2Mâ€“7M | 200â€“300 | ğŸ¦† | Low stance, first leg alternation |
| 5 | Strider | 7Mâ€“16M | 300â€“500 | ğŸƒ | Genuine coordinated walking |
| 6 | Aggressive Walker | 16Mâ€“24.9M | 350â€“670 | ğŸ’¨ | Fast but unstable, high variance |

---

## Assessment & Next Steps

### What's Working
- âœ… **LSTM memory is effective** â€” coordinated multi-step gait patterns emerge by Phase 5
- âœ… **Reward is trending upward** â€” 70 â†’ 670 (best) over 24.9M steps
- âœ… **Episode length increasing** â€” robot stays alive 2.5Ã— longer than early training
- âœ… **Classic RL learning progression** â€” the model naturally discovered standing â†’ falling exploit â†’ real walking

### What Needs Improvement
- âš ï¸ **Still unstable** â€” avg episode is 255/1000 steps, meaning the robot falls frequently
- âš ï¸ **High reward variance** â€” oscillates between 350â€“670, no convergence yet
- âš ï¸ **Speed-stability tradeoff** â€” model pushes too hard for velocity at cost of balance

### Recommendations
1. **Continue training** â€” 24.9M is early; many locomotion papers train for 100M+ steps
2. **Add stepping reward** â€” explicit bonus for foot alternation to discourage falling-as-locomotion
3. **Survival bonus** â€” small reward per timestep alive to incentivize longer episodes
4. **Curriculum learning** â€” start with low max speed, gradually increase as gait stabilizes
5. **Consider domain randomization** â€” random pushes and terrain roughness for robustness
